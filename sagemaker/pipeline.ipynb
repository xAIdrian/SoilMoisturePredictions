{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3e5dede-4204-437e-aea9-166840203259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "SOURCE_FOLDER = Path(\"src\")\n",
    "SAGEMAKER_FOLDER = Path(\"./\")\n",
    "CODE_FOLDER = Path(\"code\")\n",
    "\n",
    "CODE_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.append(f\"./{CODE_FOLDER}\")\n",
    "sys.path.append(f\"../{SOURCE_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abc3d6-65e6-4a26-853c-4a3cff5d9e36",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We'll just be importing our preprocessing class from the main project and adding it to our local SageMaker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79a429fc-cd6e-4296-a6a9-130ed03afdf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/preprocessor.py\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "SOURCE_FOLDER = Path(\"src\")\n",
    "sys.path.append(f\"../{SOURCE_FOLDER}\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pickle import dump\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import LocalOutlierFactor \n",
    "\n",
    "BASE_DIRECTORY = \"/opt/ml/processing\"\n",
    "WEATHER_DATA_FILEPATH = Path(BASE_DIRECTORY) / \"input\" / \"meteo_weather.csv\"\n",
    "SENSOR1_DATA_FILEPATH = Path(BASE_DIRECTORY) / \"input\" / \"sensor1.csv\"\n",
    "SENSOR2_DATA_FILEPATH = Path(BASE_DIRECTORY) / \"input\" / \"sensor2.csv\"\n",
    "\n",
    "def set_datetime_as_index(dataframe: pd.DataFrame, datetime_column: str):\n",
    "    dataframe[datetime_column] = pd.to_datetime(dataframe[datetime_column], format='mixed', infer_datetime_format=True)\n",
    "\n",
    "    # standardize to the format: 'Year-Month-Day Hour:Minute:Second'\n",
    "    dataframe[datetime_column] = dataframe[datetime_column].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    dataframe.set_index(datetime_column, inplace=True)\n",
    "\n",
    "    dataframe.sort_index(inplace=True)\n",
    "\n",
    "    # update the index to be datetime\n",
    "    dataframe.index = pd.to_datetime(dataframe.index)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def mark_outliers_lof(dataset, columns, n=20):\n",
    "    \"\"\"Mark values as outliers using LOF\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset\n",
    "        col (string): The column you want apply outlier detection to\n",
    "        n (int, optional): n_neighbors. Defaults to 20.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original dataframe with an extra boolean column\n",
    "        indicating whether the value is an outlier or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    lof = LocalOutlierFactor(n_neighbors=n)\n",
    "    data = dataset[columns]\n",
    "    outliers = lof.fit_predict(data)\n",
    "    X_scores = lof.negative_outlier_factor_\n",
    "\n",
    "    dataset[\"outlier_lof\"] = outliers == -1\n",
    "    return dataset, outliers, X_scores\n",
    "\n",
    "\n",
    "def _save_splits(base_directory, train, validation, test):\n",
    "    \"\"\"\n",
    "    One of the goals of this script is to output the three\n",
    "    dataset splits. This function will save each of these\n",
    "    splits to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)    \n",
    "    \n",
    "def _save_source_dataframe(base_directory, dataframe):\n",
    "    \"\"\"\n",
    "    We will take a complete dataframe prior to the test train split and save it to the directory\n",
    "    \"\"\"\n",
    "    data_path = Path(base_directory) / \"source\"\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    dataframe.to_csv(data_path / \"source.csv\", header=False, index=False)\n",
    "    \n",
    "def _save_pipeline(base_directory, pipeline):\n",
    "    \"\"\"\n",
    "    Saves the Scikit-Learn pipeline that we used to\n",
    "    preprocess the data.\n",
    "    \"\"\"\n",
    "    pipeline_path = Path(base_directory) / \"pipeline\"\n",
    "    pipeline_path.mkdir(parents=True, exist_ok=True)\n",
    "    dump(pipeline, open(pipeline_path / \"pipeline.pkl\", \"wb\"))\n",
    "\n",
    "def _save_classes(base_directory, classes):\n",
    "    \"\"\"\n",
    "    Saves the list of classes from the dataset. \n",
    "    We will need this if we ever want to use a LabelEncoder.\n",
    "    \"\"\"\n",
    "    path = Path(base_directory) / \"classes\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.asarray(classes).tofile(path / \"classes.csv\", sep=\",\")\n",
    "\n",
    "def _save_baseline(base_directory, df_train, df_test):\n",
    "    \"\"\"\n",
    "    During the data and quality monitoring steps, we will need a baseline\n",
    "    to compute constraints and statistics. This function will save that\n",
    "    baseline to the disk.\n",
    "    \"\"\"\n",
    "\n",
    "    for split, data in [(\"train\", df_train), (\"test\", df_test)]:\n",
    "        baseline_path = Path(base_directory) / f\"{split}-baseline\"\n",
    "        baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df = data.copy().dropna()\n",
    "        df.to_json(\n",
    "            baseline_path / f\"{split}-baseline.json\", orient=\"records\", lines=True\n",
    "        )\n",
    "        \n",
    "\n",
    "class FileLoader(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pattern):\n",
    "        self.pattern = pattern\n",
    "\n",
    "    def fit(self, X=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X=None):\n",
    "        load_files = glob(self.pattern)\n",
    "\n",
    "        if not load_files:\n",
    "            raise ValueError(f\"No files found for the pattern: {self.pattern}\")\n",
    "\n",
    "        load_file = load_files[0]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        print(f\"Loading: {load_file}\")\n",
    "\n",
    "        with open(load_file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "          df = pd.read_csv(file)\n",
    "          return df\n",
    "    \n",
    "class ColumnSpaceCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X.columns = X.columns.str.strip()\n",
    "        return X\n",
    "\n",
    "class DateTimeIndexSetter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, datetime_column):\n",
    "        self.datetime_column = datetime_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = set_datetime_as_index(X, self.datetime_column)\n",
    "        return X    \n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Prepare our Correlation dataframes\n",
    "# --------------------------------------------------------------\n",
    "    \n",
    "class SensorGroupBy(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_by_column):\n",
    "        self.group_by_column = group_by_column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.groupby(self.group_by_column).mean()\n",
    "        return X\n",
    "    \n",
    "class Resampler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, resample_interval):\n",
    "        self.resample_interval = resample_interval\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.resample(self.resample_interval).mean()\n",
    "        # remove the null values\n",
    "        X.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n",
    "        return X\n",
    "    \n",
    "class Saver(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        _save_source_dataframe(self.base_dir, X)  \n",
    "        return X\n",
    "    \n",
    "#--------------------------------------------------------------\n",
    "# Remove The highest values and remove outliers\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "class ExtremeValueRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name, max_value):\n",
    "        self.column_name = column_name\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X[X[self.column_name] < self.max_value]\n",
    "        return X\n",
    "\n",
    "class RemoveOutliersWithLOF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "      outlier_safe_df, outliers, X_scores = mark_outliers_lof(X, self.columns)\n",
    "      for column in self.columns:\n",
    "        outlier_safe_df.loc[outlier_safe_df['outlier_lof'], column] = np.nan\n",
    "\n",
    "      # remove the null values\n",
    "      outlier_safe_df.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n",
    "      outlier_safe_df.drop('outlier_lof', axis=1, inplace=True)\n",
    "\n",
    "      return outlier_safe_df    \n",
    "\n",
    "def preprocess_pipeline(\n",
    "    base_directory,\n",
    "    metero_data_file_path,\n",
    "    sensor1_file_path,\n",
    "    sensor2_file_path,\n",
    "    plot=False\n",
    "):\n",
    "      \n",
    "  meteo_model_pipeline = Pipeline([\n",
    "      ('file_loader', FileLoader(metero_data_file_path)),\n",
    "      ('datetime_index_setter', DateTimeIndexSetter('Date & Time')),\n",
    "      ('resampler', Resampler('15T'))\n",
    "  ])  \n",
    "\n",
    "  sensor1_pipeline = Pipeline([\n",
    "      ('file_loader', FileLoader(sensor1_file_path)),\n",
    "      ('datetime_index_setter', DateTimeIndexSetter('Date & Time')),\n",
    "      ('resampler', Resampler('15T'))\n",
    "  ]) \n",
    "\n",
    "  sensor2_pipeline = Pipeline([\n",
    "      ('file_loader', FileLoader(sensor2_file_path)),\n",
    "      ('datetime_index_setter', DateTimeIndexSetter('Date & Time')),\n",
    "      ('resampler', Resampler('15T'))\n",
    "  ]) \n",
    "\n",
    "  outlier_pipeline = Pipeline([\n",
    "      ('sensor1_extreme_value_remover', ExtremeValueRemover('Sensor1 (Ohms)', 50000)),\n",
    "      ('sensor2_extreme_value_remover', ExtremeValueRemover('Sensor2 (Ohms)', 50000)),\n",
    "      ('remove_outliers_with_lof', RemoveOutliersWithLOF(['Sensor1 (Ohms)', 'Sensor2 (Ohms)'])),\n",
    "      ('source_saver', Saver(base_directory))\n",
    "  ])\n",
    "\n",
    "  meteo_model_df = meteo_model_pipeline.fit_transform(X=None)\n",
    "  sensor1_df= sensor1_pipeline.fit_transform(X=None)        \n",
    "  sensor2_df= sensor2_pipeline.fit_transform(X=None)     \n",
    "\n",
    "  # Merge the two sensor dataframes then merge that with weather data\n",
    "  sensor_merged = pd.merge(sensor1_df, sensor2_df, left_index=True, right_index=True)\n",
    "  complete_meteo_sensor_df = pd.merge(meteo_model_df, sensor_merged, left_index=True, right_index=True, how='left')\n",
    "\n",
    "  # remove the null values\n",
    "  complete_meteo_sensor_df.interpolate(method='linear', limit_direction='forward', axis=0, inplace=True)\n",
    "  outlier_pipeline.fit_transform(X=complete_meteo_sensor_df)  \n",
    "    \n",
    "  _save_pipeline(base_directory, outlier_pipeline)\n",
    "\n",
    "  if plot:\n",
    "    plot_outliers(complete_meteo_sensor_df)\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  preprocess_pipeline(\n",
    "      BASE_DIRECTORY,\n",
    "      WEATHER_DATA_FILEPATH,\n",
    "      SENSOR1_DATA_FILEPATH,\n",
    "      SENSOR2_DATA_FILEPATH\n",
    "  )      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d8c71-5268-4f8b-a339-ef213542df76",
   "metadata": {},
   "source": [
    "## Step 2 - Testing the Preprocessing Script\n",
    "\n",
    "We can now load the script we just created and run it locally to ensure it outputs every file we need.\n",
    "\n",
    "We will set up a SageMaker Processing Job to run this script, but we always want to test the code locally. In this case, we can call the `preprocess` function with the local directory and the local copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b1b68ac-c018-41ae-ab40-a9dc397f75db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./data/meteo_data for model31.7.2023. -24.8.2023.csv\n",
      "Loading: ./data/Sensor1 data from 20.1.2023..csv\n",
      "Loading: ./data/Sensor2 data from 20.1.2023..csv\n",
      "Folders: ['source', 'pipeline']\n"
     ]
    }
   ],
   "source": [
    "from preprocessor import preprocess_pipeline\n",
    "\n",
    "with tempfile.TemporaryDirectory() as directory:\n",
    "    preprocess_pipeline(\n",
    "        base_directory=directory, \n",
    "        metero_data_file_path = './data/meteo_data for model31.7.2023. -24.8.2023.csv',\n",
    "        sensor1_file_path = './data/Sensor1 data from 20.1.2023..csv',\n",
    "        sensor2_file_path = './data/Sensor2 data from 20.1.2023..csv'\n",
    "    )\n",
    "    \n",
    "    print(f\"Folders: {os.listdir(directory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf5c08-3cbb-4776-a1f5-d601ba196f1f",
   "metadata": {},
   "source": [
    "## Pipeline Configuration\n",
    "\n",
    "As you saw above we are declaring file locations for our files and such to be stored in our Docker instance that SageMaker will reference. We tested locally but now we need our remote files to be moved into the pipeline.  This is done with S3.\n",
    "\n",
    "We'll be compiling a series of parameter objects that will then be passed to our SageMaker instance which already has a pipeline saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0c654fb-425b-41f0-9758-ee0bbbb7358c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "from constants import *\n",
    "\n",
    "weather_location = ParameterString(\n",
    "    name=\"weather_location\",\n",
    "    default_value=f\"{S3_LOCATION}/meteo_data for model 30.1.2023. - 31.7.2023..csv\",\n",
    ")\n",
    "\n",
    "sensor1_location = ParameterString(\n",
    "    name=\"sensor1_location\",\n",
    "    default_value=f\"{S3_LOCATION}/Sensor1 data 21.1.2023..csv\",\n",
    ")\n",
    "\n",
    "sensor2_location = ParameterString(\n",
    "    name=\"sensor2_location\",\n",
    "    default_value=f\"{S3_LOCATION}/Sensor2 data 21.1.2023..csv\",\n",
    ")\n",
    "\n",
    "preprocessor_destination = ParameterString(\n",
    "    name=\"preprocessor_destination\",\n",
    "    default_value=f\"{S3_LOCATION}/preprocessing\",\n",
    ")\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "# Important: Do not rerun successful steps. Reuse outputs of step if completed within 15 days.\n",
    "cache_config = CacheConfig(\n",
    "    enable_caching=True, \n",
    "    expire_after=\"15d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b2dc99-cebb-4579-baa8-8e145fe7fb2c",
   "metadata": {},
   "source": [
    "## Processing Step\n",
    "\n",
    "This Processing Step will create a SageMaker Processing Job in the background, run the script, and upload the output to S3. You can use Processing Jobs to perform data preprocessing, post-processing, feature engineering, data validation, and model evaluation. \n",
    "\n",
    "ProcessingStep requires a list of inputs that we need on the preprocessing script. In this case, the input is the dataset we stored in S3. We also have a few outputs that we want SageMaker to capture when the Processing Job finishes. SageMaker will upload every one of these outputs to the location specified by the `preprocessor_destination` parameter except the baseline data, which we will upload to the location specified by the `baseline_destination` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "300978f0-6468-4346-a803-9bde38cec1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soilpreds/Sensor1 data 21.1.2023..csv\n",
      "soilpreds/Sensor2 data 21.1.2023..csv\n",
      "soilpreds/accuweather_hourly_1.29_to_6.15.csv\n",
      "soilpreds/meteo_data for model 30.1.2023. - 31.7.2023..csv\n",
      "soilpreds/meteo_data_for_accuweather_comparison_1.30_to_6.15.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'default-soil-predictions'\n",
    "result = s3.list_objects(Bucket=bucket_name)\n",
    "\n",
    "# Iterate over the objects in the bucket and print them\n",
    "for content in result.get('Contents', []):\n",
    "    print(content['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d13e8949-cc9e-4a80-9d76-b62cacb14206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Processor is our \"service\" or long-running job that contains the container image\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=\"soil-moisture-preprocessing\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# Create the step object for the greater Pipeline hosted on SageMaker\n",
    "preprocess_data_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=weather_location, destination=\"/opt/ml/processing/weather_input\"),\n",
    "        ProcessingInput(source=sensor1_location, destination=\"/opt/ml/processing/sensor1_input\"),\n",
    "        ProcessingInput(source=sensor2_location, destination=\"/opt/ml/processing/sensor2_input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"source\", source=\"/opt/ml/processing/source\"),\n",
    "        ProcessingOutput(output_name=\"pipeline\", source=\"/opt/ml/processing/pipeline\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"classes\", source=\"/opt/ml/processing/classes\", destination=preprocessor_destination),\n",
    "        ProcessingOutput(output_name=\"train-baseline\", source=\"/opt/ml/processing/train-baseline\"),\n",
    "        ProcessingOutput(output_name=\"test-baseline\", source=\"/opt/ml/processing/test-baseline\"),\n",
    "    ],\n",
    "    code=f\"{CODE_FOLDER}/preprocessor.py\",\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f338453-8eca-482d-9a4f-5ebcfb4d319c",
   "metadata": {},
   "source": [
    "## Setting up the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89bda6d8-06b1-4e6f-ab3e-f1249accce8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:294404188698:pipeline/preprocessing-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '55ab2ef9-2ed6-4df6-9769-44be5e3ec575',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '55ab2ef9-2ed6-4df6-9769-44be5e3ec575',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '90',\n",
       "   'date': 'Fri, 01 Sep 2023 09:44:40 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline\",\n",
    "    parameters=[\n",
    "        weather_location,\n",
    "        sensor1_location,\n",
    "        sensor2_location,\n",
    "        preprocessor_destination,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocess_data_step, \n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00d23d10-89f2-4a2e-8cf0-8212e4f97c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:294404188698:pipeline/preprocessing-pipeline/execution/d51l04a2spua', sagemaker_session=<sagemaker.session.Session object at 0x7fa9a7353a90>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a299d7-b797-47cb-8263-fdc7d75b90d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-gpu-py38-cu112-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:294404188698:studio-lifecycle-config/packages"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
